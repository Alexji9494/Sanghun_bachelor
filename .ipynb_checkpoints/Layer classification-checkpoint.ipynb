{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1g3ZM0d_KIbL"
   },
   "source": [
    "Layer classification of Undergrad's thesis by Sanghun Jee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tj81s-VNTY-J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tables\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, ward\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy import stats\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MEw2stx0Cx9e",
    "outputId": "b27da0da-82a5-4796-fff6-8d47d82214bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/undergrad thesis/mouse_VISp_gene_expression_matrices_2018-06-14\n"
     ]
    }
   ],
   "source": [
    "cd drive/undergrad thesis/mouse_VISp_gene_expression_matrices_2018-06-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idsan\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\idsan\\Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TDwD_eDBOxY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log2\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#We will load the exon dataset and sample file\n",
    "exon_filtered = pd.read_hdf('exon_filtered.h5', key='exon_filtered')\n",
    "exon_filtered_log2 = np.log2(exon_filtered)\n",
    "del exon_filtered\n",
    "samples = pd.read_csv('mouse_VISp_2018-06-14_samples-columns.csv')\n",
    "exon_filtered_log2[exon_filtered_log2==exon_filtered_log2.values[0, 0]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv-dLvammfhX"
   },
   "outputs": [],
   "source": [
    "#Layer indices\n",
    "L1_ind = np.where(samples.values[:, 13] =='L1')[0]\n",
    "L23_ind = np.where(samples.values[:, 13] =='L2/3')[0]\n",
    "L4_ind = np.where(samples.values[:, 13] =='L4')[0]\n",
    "L5_ind = np.where(samples.values[:, 13] =='L5')[0]\n",
    "L6_ind = np.where(samples.values[:, 13] =='L6')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UAwnk-xjRC2"
   },
   "outputs": [],
   "source": [
    "#Layer data\n",
    "L1_log2 = exon_filtered_log2.iloc[:, L1_ind]\n",
    "L23_log2 = exon_filtered_log2.iloc[:, L23_ind]\n",
    "L4_log2 = exon_filtered_log2.iloc[:, L4_ind]\n",
    "L5_log2 = exon_filtered_log2.iloc[:, L5_ind]\n",
    "L6_log2 = exon_filtered_log2.iloc[:, L6_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzPp0yOfONRF"
   },
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heV4H24JaLpI"
   },
   "outputs": [],
   "source": [
    "#I already drew dendrogram and decided threshold\n",
    "threshold = 300\n",
    "clustering_L1 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L1_log2)\n",
    "clustering_L23 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L23_log2)\n",
    "clustering_L4 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L4_log2)\n",
    "clustering_L5 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L5_log2)\n",
    "clustering_L6 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L6_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjkKdOMY7YYC"
   },
   "outputs": [],
   "source": [
    "# now we know which genes are in which label\n",
    "L1_labels = clustering_L1.labels_\n",
    "L23_labels = clustering_L23.labels_\n",
    "L4_labels = clustering_L4.labels_\n",
    "L5_labels = clustering_L5.labels_\n",
    "L6_labels = clustering_L6.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8RakTyC8q6e"
   },
   "outputs": [],
   "source": [
    "pre_L1_labels = []\n",
    "pre_L23_labels = []\n",
    "pre_L4_labels = []\n",
    "pre_L5_labels = []\n",
    "pre_L6_labels = []\n",
    "for i in range(0, 5295):\n",
    "  pre_L1_labels.append((L1_labels[i], i))\n",
    "  pre_L23_labels.append((L23_labels[i], i))\n",
    "  pre_L4_labels.append((L4_labels[i], i))\n",
    "  pre_L5_labels.append((L5_labels[i], i))\n",
    "  pre_L6_labels.append((L6_labels[i], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnZ9lFs9JgUA"
   },
   "outputs": [],
   "source": [
    "pre_L1_labels = np.reshape(pre_L1_labels, (-1, 2))\n",
    "pre_L23_labels = np.reshape(pre_L23_labels, (-1, 2))\n",
    "pre_L4_labels = np.reshape(pre_L4_labels, (-1, 2))\n",
    "pre_L5_labels = np.reshape(pre_L5_labels, (-1, 2))\n",
    "pre_L6_labels = np.reshape(pre_L6_labels, (-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7efA632J0iC"
   },
   "outputs": [],
   "source": [
    "total_pre = [pre_L1_labels, pre_L23_labels, pre_L4_labels, pre_L5_labels, pre_L6_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wFRAMrF9JVt"
   },
   "outputs": [],
   "source": [
    "#So we gotta make a list that ((layer_num), clustered_num, clustered_points)\n",
    "gene_group = []\n",
    "layer_num = []\n",
    "for j in range(0, len(total_pre)):\n",
    "  max = np.max(total_pre[j][:, 0]).astype(int)\n",
    "  layer_num.append([j, max])\n",
    "  for i in range(0, max):\n",
    "    gene_group.append((i, np.where(total_pre[j][:, 0] == i)))\n",
    "gene_group = np.reshape(gene_group, (-1, 2))\n",
    "layer_num = np.reshape(layer_num, (-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZJRIXcnBWEk"
   },
   "outputs": [],
   "source": [
    "#Let's make a dictionary\n",
    "#summmary[layer, cluster] = the gene index\n",
    "summary = {}\n",
    "gene_group_ind = 0\n",
    "for i in layer_num[:, 0]:\n",
    "  for j in range(0, layer_num[i, 1]):\n",
    "    summary[i, j] = gene_group[gene_group_ind, 1][0]\n",
    "    gene_group_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVSNoRXl7mOY"
   },
   "outputs": [],
   "source": [
    "#make mean value dataframe of each clustered geneset\n",
    "L1, L23, L4, L5, L6 = {}, {}, {}, {}, {}\n",
    "\n",
    "for i in range(0, layer_num[0, 1]):\n",
    "  L1[i] = pd.DataFrame(np.mean(L1_log2.values[summary[0, i], :], axis=0))\n",
    "for i in range(0, layer_num[1, 1]):\n",
    "  L23[i] = pd.DataFrame(np.mean(L23_log2.values[summary[1, i], :], axis=0))\n",
    "for i in range(0, layer_num[2, 1]):\n",
    "  L4[i] = pd.DataFrame(np.mean(L4_log2.values[summary[2, i], :], axis=0))\n",
    "for i in range(0, layer_num[3, 1]):\n",
    "  L5[i] = pd.DataFrame(np.mean(L5_log2.values[summary[3, i], :], axis=0))\n",
    "for i in range(0, layer_num[4, 1]):\n",
    "  L6[i] = pd.DataFrame(np.mean(L6_log2.values[summary[4, i], :], axis=0))\n",
    "\n",
    "L1_chi = pd.concat(L1, axis=1)\n",
    "L23_chi = pd.concat(L23, axis=1)\n",
    "L4_chi = pd.concat(L4, axis=1)\n",
    "L5_chi = pd.concat(L5, axis=1)\n",
    "L6_chi = pd.concat(L6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViIBAKGz-6Ve"
   },
   "outputs": [],
   "source": [
    "#Statistical test\n",
    "_, p_L1 = stats.chisquare(L1_chi, ddof= layer_num[0, 1]-1)\n",
    "_, p_L23 = stats.chisquare(L23_chi, ddof= layer_num[1, 1]-1)\n",
    "_, p_L4 = stats.chisquare(L4_chi, ddof= layer_num[2, 1]-1)\n",
    "_, p_L5 = stats.chisquare(L5_chi, ddof= layer_num[3, 1]-1)\n",
    "_, p_L6 = stats.chisquare(L6_chi, ddof= layer_num[4, 1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi6RhZS1jhfM"
   },
   "outputs": [],
   "source": [
    "#row = samples, col = genes sets.\n",
    "signi_L1 = pd.DataFrame(L1_chi.values[:, np.logical_and(p_L1>0, p_L1<0.001)])\n",
    "signi_L23 = pd.DataFrame(L23_chi.values[:, np.logical_and(p_L23>0, p_L23<0.001)])\n",
    "signi_L4 = pd.DataFrame(L4_chi.values[:, np.logical_and(p_L4>0, p_L4<0.001)])\n",
    "signi_L5 = pd.DataFrame(L5_chi.values[:, np.logical_and(p_L5>0, p_L5<0.001)])\n",
    "signi_L6 = pd.DataFrame(L6_chi.values[:, np.logical_and(p_L6>0, p_L6<0.001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OF3g8YQqXRU"
   },
   "outputs": [],
   "source": [
    "total_exon = [L1_log2, L23_log2, L4_log2, L5_log2, L6_log2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLTyCM3C4e_t"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "#Let's get which clustered things would be significantly represented the each layer\n",
    "min_num = np.min([signi_L1.shape[0], signi_L23.shape[0], signi_L4.shape[0], signi_L5.shape[0], signi_L6.shape[0]])\n",
    "L23_sample_num = np.random.choice(signi_L23.shape[0], size=min_num, replace=False)\n",
    "L4_sample_num = np.random.choice(signi_L4.shape[0], size=min_num, replace=False)\n",
    "L5_sample_num = np.random.choice(signi_L5.shape[0], size=min_num, replace=False)\n",
    "L6_sample_num = np.random.choice(signi_L6.shape[0], size=min_num, replace=False)\n",
    "\n",
    "p_value_overall = []\n",
    "for i in itertools.product(*[np.arange(signi_L1.shape[1]), np.arange(signi_L23.shape[1]), np.arange(signi_L4.shape[1]), np.arange(signi_L5.shape[1]), np.arange(signi_L6.shape[1])]):\n",
    "  candi = pd.DataFrame(np.vstack((signi_L1.values[:, i[0]], signi_L23.values[L23_sample_num, i[1]], signi_L4.values[L4_sample_num, i[2]], signi_L5.values[L5_sample_num, i[3]], signi_L6.values[L6_sample_num, i[4]]))).T\n",
    "  _, p_candi = stats.chisquare(candi, ddof=4)\n",
    "  if np.count_nonzero(p_candi<0.001) == 5:\n",
    "    p_value_overall.append((i, p_candi))\n",
    "    #print('p_value_num : {}, total: {}'.format(len(p_value_overall), i))\n",
    "p_value_overall = np.reshape(p_value_overall, (-1, 2, 5))\n",
    "#p_value_overall = np.reshape(p_value_overall, (-1, 2))\n",
    "#np.savetxt('p_value_overal.txt', p_value_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for control test which means there is no statistical test so that there is a lot of noise to learn whether cell is in certain layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L84zLJ1YPJwX",
    "outputId": "827c1d08-3031-4943-f2d1-918cd86b7189"
   },
   "outputs": [],
   "source": [
    "total_control_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in range(0, len(p_L1)):\n",
    "  total_control_test[num] = np.mean(exon_filtered_log2.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in range(0, len(p_L23)):\n",
    "  total_control_test[num] = np.mean(exon_filtered_log2.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in range(0, len(p_L4)):\n",
    "  total_control_test[num] = np.mean(exon_filtered_log2.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in range(0, len(p_L5)):\n",
    "  total_control_test[num] = np.mean(exon_filtered_log2.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in range(0, len(p_L6)):\n",
    "  total_control_test[num] = np.mean(exon_filtered_log2.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "total_control_test = pd.DataFrame(total_control_test)\n",
    "#num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for test. So the geneset is filtered by statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5n7pEHI7Ksu6",
    "outputId": "9c045a2b-9132-448f-aeef-073bcc094aba"
   },
   "outputs": [],
   "source": [
    "total_real_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  total_real_test[num] = np.mean(exon_filtered_log2.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  total_real_test[num] = np.mean(exon_filtered_log2.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  total_real_test[num] = np.mean(exon_filtered_log2.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  total_real_test[num] = np.mean(exon_filtered_log2.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  total_real_test[num] = np.mean(exon_filtered_log2.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "total_real_test = pd.DataFrame(total_real_test)\n",
    "#num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vRWO39Rss5g"
   },
   "outputs": [],
   "source": [
    "labels = np.zeros((len(total_real_test)))\n",
    "labels[L1_ind] = 1\n",
    "labels[L23_ind] = 2\n",
    "labels[L4_ind] = 3\n",
    "labels[L5_ind] =4\n",
    "labels[L6_ind] = 5\n",
    "labels= pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIwEQlLAt9XO"
   },
   "outputs": [],
   "source": [
    "labels_real = pd.DataFrame(labels.values[np.where(labels>0)[0]].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDHKqhNdHj_H"
   },
   "outputs": [],
   "source": [
    "total_real_test = total_real_test.values[labels_real[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90CfXNsFP3Nh"
   },
   "outputs": [],
   "source": [
    "total_control_test = total_control_test.values[labels_real[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7ebOdkqJt11"
   },
   "outputs": [],
   "source": [
    "labels_real = pd.DataFrame(labels_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqMMMGfztj1y"
   },
   "outputs": [],
   "source": [
    "# I will split origianl set to training, test set\n",
    "#for control or test set, I should change the code at the time.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(total_real_test, labels_real, test_size = .15, train_size = .85)\n",
    "from keras.utils import to_categorical\n",
    "y_train_binary_real = to_categorical(y_train_real)\n",
    "y_test_binary_real = to_categorical(y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-2d1ead0bb051>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-2d1ead0bb051>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    import tensorflow-gpu as tf\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow-gpu as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMJZEIpauzkJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.backend import tensorflow_backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 24780
    },
    "colab_type": "code",
    "id": "YptdW01ftR7i",
    "outputId": "3816c000-99e6-4b24-fb36-fb839e7d39e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1205 20:24:05.820257 24944 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1205 20:24:05.980813 24944 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 9306)              325710    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9306)              37224     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9306)              86610942  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9306)              37224     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 55842     \n",
      "=================================================================\n",
      "Total params: 87,066,942\n",
      "Trainable params: 87,029,718\n",
      "Non-trainable params: 37,224\n",
      "_________________________________________________________________\n",
      "Train on 8375 samples, validate on 931 samples\n",
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train_real.shape[0], input_shape=(X_train_real.shape[1],), activation='sigmoid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(Dense(X_train_real.shape[0], kernel_regularizer=regularizers.l2(0.001), activation='sigmoid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(Dense(y_train_binary.shape[0]//4, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(y_train_binary.shape[0]//8, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(y_train_binary.shape[0]//16, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "#model.add(Dense(y_train_binary.shape[0]//16, activation='relu'))\n",
    "\n",
    "model.add(Dense(y_train_binary_real.shape[1], activation='softmax'))\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer= Adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "h = model.fit(X_train_real, y_train_binary_real, epochs = 1000, batch_size = X_train_real.shape[0], verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each significant geneset heatmap\n",
    "fig, _= plt.subplots(figsize=(30, 15))\n",
    "ax1 = fig.add_subplot(1, 5, 1)\n",
    "ax1 = sns.heatmap(signi_L1, vmin = 0, vmax = 15, cbar = False, cmap=\"YlGnBu\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 5, 2)\n",
    "ax2 = sns.heatmap(signi_L23, vmin = 0, vmax = 15, cbar = False, cmap=\"YlGnBu\")\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(1, 5, 3)\n",
    "ax3 = sns.heatmap(signi_L4, vmin = 0, vmax = 15, cbar = False, cmap=\"YlGnBu\")\n",
    "\n",
    "ax4 = fig.add_subplot(1, 5, 4)\n",
    "ax4 = sns.heatmap(signi_L5, vmin = 0, vmax = 15, cbar = False, cmap=\"YlGnBu\")\n",
    "\n",
    "ax5 = fig.add_subplot(1, 5, 5)\n",
    "ax5 = sns.heatmap(signi_L6, vmin = 0, vmax = 15, cmap=\"YlGnBu\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "DpPkY0yW5Edb",
    "outputId": "d2b638bb-13f6-41d4-dd9a-6e66a624cf9f"
   },
   "outputs": [],
   "source": [
    "# Does this model predict well?\n",
    "pred = model.predict(X_test_real, batch_size=X_test_real.shape[0], verbose=1, steps=None)\n",
    "np.where(pred[0] == np.max(pred[0]))\n",
    "predict = []\n",
    "max_label = np.max(pred, axis=1)\n",
    "for i in range(0, len(max_label)):\n",
    "  predict.append(np.where(pred[i] == max_label[i]))\n",
    "predict = np.reshape(predict, (1, -1))[0]\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_897KfxNcTU"
   },
   "outputs": [],
   "source": [
    "#Save the test set model and results\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_test.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "history = h\n",
    "# Plot training & validation accuracy values\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(history.history['acc'])\n",
    "ax.plot(history.history['val_acc'])\n",
    "ax.set_title('Model accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Test'], loc='upper left')\n",
    "fig.savefig('Layer_classification accuracy_test.png')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 10))\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_title('Model loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(['Train', 'Test'], loc='upper left')\n",
    "fig1.savefig('Layer_classification loss_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the control set model and results\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_control.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "history = h\n",
    "# Plot training & validation accuracy values\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(history.history['acc'])\n",
    "ax.plot(history.history['val_acc'])\n",
    "ax.set_title('Model accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Test'], loc='upper left')\n",
    "fig.savefig('Layer_classification accuracy_control.png')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 10))\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_title('Model loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(['Train', 'Test'], loc='upper left')\n",
    "fig1.savefig('Layer_classification loss_control.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cells are for testing interlayer_cell like L1_L23. Not completed yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Y08vvC1ZHCC"
   },
   "outputs": [],
   "source": [
    "L1_L23_ind = np.where(samples.iloc[:, 13] == 'L1-L2/3')[0]\n",
    "L6b_ind = np.where(samples.iloc[:, 13] == 'L6b')[0]\n",
    "L1_L4_ind = np.where(samples.iloc[:, 13] == 'L1-L4')[0]\n",
    "L4_L5_ind = np.where(samples.iloc[:, 13] == 'L4-L5')[0]\n",
    "L5_L6_ind = np.where(samples.iloc[:, 13] == 'L5-L6')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6Rf_c5VZQiK"
   },
   "outputs": [],
   "source": [
    "L1_L23 = pd.DataFrame(exon_filtered_log2.values[:, L1_L23_ind])\n",
    "L1_L4 = pd.DataFrame(exon_filtered_log2.values[:, L1_L4_ind])\n",
    "L4_L5 = pd.DataFrame(exon_filtered_log2.values[:, L4_L5_ind])\n",
    "L5_L6 = pd.DataFrame(exon_filtered_log2.values[:, L5_L6_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7nMpXficNf1"
   },
   "outputs": [],
   "source": [
    "L6b = pd.DataFrame(exon_filtered_log2.values[:, L6b_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zG3iuby9xyg2",
    "outputId": "ff327377-ad49-4725-dd58-d334f82a6e6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L5_L6_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  L5_L6_test[num] = np.mean(L5_L6.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  L5_L6_test[num] = np.mean(L5_L6.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  L5_L6_test[num] = np.mean(L5_L6.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  L5_L6_test[num] = np.mean(L5_L6.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  L5_L6_test[num] = np.mean(L5_L6.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "L5_L6_test = pd.DataFrame(L5_L6_test)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3fuzjNOCqf3U",
    "outputId": "955bd543-7823-4951-c639-1836ee1f6f92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L6b_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  L6b_test[num] = np.mean(L6b.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  L6b_test[num] = np.mean(L6b.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  L6b_test[num] = np.mean(L6b.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  L6b_test[num] = np.mean(L6b.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  L6b_test[num] = np.mean(L6b.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "L6b_test = pd.DataFrame(L6b_test)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Yyny43bJqGts",
    "outputId": "5d9f83e0-d007-4516-b01c-79fabbedb0d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4_L5_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  L4_L5_test[num] = np.mean(L4_L5.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  L4_L5_test[num] = np.mean(L4_L5.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  L4_L5_test[num] = np.mean(L4_L5.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  L4_L5_test[num] = np.mean(L4_L5.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  L4_L5_test[num] = np.mean(L4_L5.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "L4_L5_test = pd.DataFrame(L4_L5_test)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RGC-EiqwokrB",
    "outputId": "388026e5-f703-441e-bef8-8bc873dc06c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_L4_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  L1_L4_test[num] = np.mean(L1_L4.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  L1_L4_test[num] = np.mean(L1_L4.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  L1_L4_test[num] = np.mean(L1_L4.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  L1_L4_test[num] = np.mean(L1_L4.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  L1_L4_test[num] = np.mean(L1_L4.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "L1_L4_test = pd.DataFrame(L1_L4_test)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m32nf5LwZiGT",
    "outputId": "76781645-97d4-4488-a1c2-3d01e8e4c0c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_L23_test = {}\n",
    "num = 0\n",
    "\n",
    "for i in np.nonzero(np.logical_and(p_L1>0, p_L1<0.005))[0]:\n",
    "  L1_L23_test[num] = np.mean(L1_L23.iloc[summary[0, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L23>0, p_L23<0.005))[0]:\n",
    "  L1_L23_test[num] = np.mean(L1_L23.iloc[summary[1, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L4>0, p_L4<0.005))[0]:\n",
    "  L1_L23_test[num] = np.mean(L1_L23.iloc[summary[2, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L5>0, p_L5<0.005))[0]:\n",
    "  L1_L23_test[num] = np.mean(L1_L23.iloc[summary[3, i], :], axis=0)\n",
    "  num += 1\n",
    "for i in np.nonzero(np.logical_and(p_L6>0, p_L6<0.005))[0]:\n",
    "  L1_L23_test[num] = np.mean(L1_L23.iloc[summary[4, i], :], axis=0)\n",
    "  num += 1\n",
    "\n",
    "L1_L23_test = pd.DataFrame(L1_L23_test)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "avcPl51xZx-k",
    "outputId": "de95dec9-4b55-4e58-b740-4f2ede8bf749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1007/1007 [==============================] - 0s 76us/step\n"
     ]
    }
   ],
   "source": [
    "#L1_L23 prediction test\n",
    "X = L5_L6_test\n",
    "pred = model.predict(X, batch_size=X.shape[0], verbose=1, steps=None)\n",
    "np.where(pred[0] == np.max(pred[0]))\n",
    "predict = []\n",
    "max_label = np.max(pred, axis=1)\n",
    "for i in range(0, len(max_label)):\n",
    "  predict.append(np.where(pred[i] == max_label[i]))\n",
    "predict = np.reshape(predict, (1, -1))[0]\n",
    "#pred = np.reshape(pred, (-1, 5))\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tables\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, ward\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#cd drive/undergrad thesis/mouse_VISp_gene_expression_matrices_2018-06-14\n",
    "exon_filtered = pd.read_hdf('exon_filtered.h5', key='exon_filtered')\n",
    "exon_filtered_log2 = np.log2(exon_filtered)\n",
    "del exon_filtered\n",
    "samples = pd.read_csv('mouse_VISp_2018-06-14_samples-columns.csv')\n",
    "exon_filtered_log2[exon_filtered_log2==exon_filtered_log2.values[0, 0]]=0\n",
    "L1_ind = np.where(samples.values[:, 13] =='L1')[0]\n",
    "L23_ind = np.where(samples.values[:, 13] =='L2/3')[0]\n",
    "L4_ind = np.where(samples.values[:, 13] =='L4')[0]\n",
    "L5_ind = np.where(samples.values[:, 13] =='L5')[0]\n",
    "L6_ind = np.where(samples.values[:, 13] =='L6')[0]\n",
    "L1_log2 = exon_filtered_log2.iloc[:, L1_ind]\n",
    "L23_log2 = exon_filtered_log2.iloc[:, L23_ind]\n",
    "L4_log2 = exon_filtered_log2.iloc[:, L4_ind]\n",
    "L5_log2 = exon_filtered_log2.iloc[:, L5_ind]\n",
    "L6_log2 = exon_filtered_log2.iloc[:, L6_ind]\n",
    "\n",
    "#Clustering\n",
    "threshold = 300\n",
    "clustering_L1 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L1_log2)\n",
    "clustering_L23 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L23_log2)\n",
    "clustering_L4 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L4_log2)\n",
    "clustering_L5 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L5_log2)\n",
    "clustering_L6 = AgglomerativeClustering(n_clusters=None, linkage='ward', distance_threshold= threshold).fit(L6_log2)\n",
    "\n",
    "L1_labels = clustering_L1.labels_\n",
    "L23_labels = clustering_L23.labels_\n",
    "L4_labels = clustering_L4.labels_\n",
    "L5_labels = clustering_L5.labels_\n",
    "L6_labels = clustering_L6.labels_\n",
    "\n",
    "pre_L1_labels = []\n",
    "pre_L23_labels = []\n",
    "pre_L4_labels = []\n",
    "pre_L5_labels = []\n",
    "pre_L6_labels = []\n",
    "for i in range(0, 5295):\n",
    "  pre_L1_labels.append((L1_labels[i], i))\n",
    "  pre_L23_labels.append((L23_labels[i], i))\n",
    "  pre_L4_labels.append((L4_labels[i], i))\n",
    "  pre_L5_labels.append((L5_labels[i], i))\n",
    "  pre_L6_labels.append((L6_labels[i], i))\n",
    "\n",
    "pre_L1_labels = np.reshape(pre_L1_labels, (-1, 2))\n",
    "pre_L23_labels = np.reshape(pre_L23_labels, (-1, 2))\n",
    "pre_L4_labels = np.reshape(pre_L4_labels, (-1, 2))\n",
    "pre_L5_labels = np.reshape(pre_L5_labels, (-1, 2))\n",
    "pre_L6_labels = np.reshape(pre_L6_labels, (-1, 2))\n",
    "\n",
    "total_pre = [pre_L1_labels, pre_L23_labels, pre_L4_labels, pre_L5_labels, pre_L6_labels]\n",
    "#So we gotta make a list that ((layer_num), clustered_num, clustered_points)\n",
    "gene_group = []\n",
    "layer_num = []\n",
    "for j in range(0, len(total_pre)):\n",
    "  max = np.max(total_pre[j][:, 0]).astype(int)\n",
    "  layer_num.append([j, max])\n",
    "  for i in range(0, max):\n",
    "    gene_group.append((i, np.where(total_pre[j][:, 0] == i)))\n",
    "gene_group = np.reshape(gene_group, (-1, 2))\n",
    "layer_num = np.reshape(layer_num, (-1, 2))\n",
    "\n",
    "#Let's make a dictionary\n",
    "#summmary[layer, cluster] = the gene index\n",
    "summary = {}\n",
    "gene_group_ind = 0\n",
    "for i in layer_num[:, 0]:\n",
    "  for j in range(0, layer_num[i, 1]):\n",
    "    summary[i, j] = gene_group[gene_group_ind, 1][0]\n",
    "    gene_group_ind += 1\n",
    "    \n",
    "L1, L23, L4, L5, L6 = {}, {}, {}, {}, {}\n",
    "\n",
    "for i in range(0, layer_num[0, 1]):\n",
    "  L1[i] = pd.DataFrame(np.mean(L1_log2.values[summary[0, i], :], axis=0))\n",
    "for i in range(0, layer_num[1, 1]):\n",
    "  L23[i] = pd.DataFrame(np.mean(L23_log2.values[summary[1, i], :], axis=0))\n",
    "for i in range(0, layer_num[2, 1]):\n",
    "  L4[i] = pd.DataFrame(np.mean(L4_log2.values[summary[2, i], :], axis=0))\n",
    "for i in range(0, layer_num[3, 1]):\n",
    "  L5[i] = pd.DataFrame(np.mean(L5_log2.values[summary[3, i], :], axis=0))\n",
    "for i in range(0, layer_num[4, 1]):\n",
    "  L6[i] = pd.DataFrame(np.mean(L6_log2.values[summary[4, i], :], axis=0))\n",
    "  \n",
    "L1_chi = pd.concat(L1, axis=1)\n",
    "L23_chi = pd.concat(L23, axis=1)\n",
    "L4_chi = pd.concat(L4, axis=1)\n",
    "L5_chi = pd.concat(L5, axis=1)\n",
    "L6_chi = pd.concat(L6, axis=1)\n",
    "\n",
    "_, p_L1 = stats.chisquare(L1_chi, ddof= layer_num[0, 1]-1)\n",
    "_, p_L23 = stats.chisquare(L23_chi, ddof= layer_num[1, 1]-1)\n",
    "_, p_L4 = stats.chisquare(L4_chi, ddof= layer_num[2, 1]-1)\n",
    "_, p_L5 = stats.chisquare(L5_chi, ddof= layer_num[3, 1]-1)\n",
    "_, p_L6 = stats.chisquare(L6_chi, ddof= layer_num[4, 1]-1)\n",
    "\n",
    "#row = samples, col = genes sets.\n",
    "signi_L1 = pd.DataFrame(L1_chi.values[:, p_L1<0.001])\n",
    "signi_L23 = pd.DataFrame(L23_chi.values[:, p_L23<0.001])\n",
    "signi_L4 = pd.DataFrame(L4_chi.values[:, p_L4<0.001])\n",
    "signi_L5 = pd.DataFrame(L5_chi.values[:, p_L5<0.001])\n",
    "signi_L6 = pd.DataFrame(L6_chi.values[:, p_L6<0.001])\n",
    "\n",
    "\n",
    "#Let's figure out which clustered things would significantly represent the each layer\n",
    "min_num = np.min([signi_L1.shape[0], signi_L23.shape[0], signi_L4.shape[0], signi_L5.shape[0], signi_L6.shape[0]])\n",
    "L1_sample_num = np.random.choice(signi_L1.shape[0], size=min_num, replace=False)\n",
    "L23_sample_num = np.random.choice(signi_L23.shape[0], size=min_num, replace=False)\n",
    "L4_sample_num = np.random.choice(signi_L4.shape[0], size=min_num, replace=False)\n",
    "L5_sample_num = np.random.choice(signi_L5.shape[0], size=min_num, replace=False)\n",
    "L6_sample_num = np.random.choice(signi_L6.shape[0], size=min_num, replace=False)\n",
    "\n",
    "p_value_overall = []\n",
    "for i in itertools.product(*[np.arange(signi_L1.shape[1]), np.arange(signi_L23.shape[1]), np.arange(signi_L4.shape[1]), np.arange(signi_L5.shape[1]), np.arange(signi_L6.shape[1])]):\n",
    "  candi = pd.DataFrame(np.vstack((signi_L1.values[:, i[0]], signi_L23.values[L23_sample_num, i[1]], signi_L4.values[L4_sample_num, i[2]], signi_L5.values[L5_sample_num, i[3]], signi_L6.values[L6_sample_num, i[4]]))).T\n",
    "  _, p_candi = stats.chisquare(candi, ddof=4)\n",
    "  if np.count_nonzero(p_candi<0.001) == 5:\n",
    "    p_value_overall.append((i, p_candi))\n",
    "    #print('p_value_num : {}, total: {}'.format(len(p_value_overall), i))\n",
    "p_value_overall = np.reshape(p_value_overall, (-1, 2, 5))\n",
    "#p_value_overall = np.reshape(p_value_overall, (-1, 2))\n",
    "#np.savetxt('p_value_overal.txt', p_value_overall)\n",
    "\n",
    "#Let's get the significant(p<0.001) geneset for each layer\n",
    "p_L1_clust = np.unique(p_value_overall[:, 0, 0])\n",
    "p_L23_clust = np.unique(p_value_overall[:, 0, 1])\n",
    "p_L4_clust = np.unique(p_value_overall[:, 0, 2])\n",
    "p_L5_clust = np.unique(p_value_overall[:, 0, 3])\n",
    "p_L6_clust = np.unique(p_value_overall[:, 0, 4])\n",
    "p_total_clust = [p_L1_clust, p_L23_clust, p_L4_clust, p_L5_clust, p_L6_clust]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "undergrad.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
